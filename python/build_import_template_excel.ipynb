{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:489: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector', 'key_varreqs_all'\n",
      "  key_dict = f\"unit_energy_to_energy_equivalent_{me}\"\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:489: UserWarning: Invalid subsector attribute 'key_varreqs_all'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector'\n",
      "  key_dict = f\"unit_energy_to_energy_equivalent_{me}\"\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:489: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector'\n",
      "  key_dict = f\"unit_energy_to_energy_equivalent_{me}\"\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:937: UserWarning: clean_partial_category_dictionary: Invalid categories values 'domestic_rural_ww', 'domestic_urban_ww' dropped when cleaning the dictionary. Category values not found.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'setup_analysis' from '/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/setup_analysis.py'>"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import model_afolu as ma\n",
    "import model_circular_economy as mc\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import data_structures as ds\n",
    "import setup_analysis as sa\n",
    "import support_functions as sf\n",
    "import importlib\n",
    "import warnings\n",
    "importlib.reload(ds)\n",
    "importlib.reload(sa)\n",
    "\n",
    "\n",
    "#import data_structures as ds\n",
    "#importlib.reload(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ce = mc.CircularEconomy(sa.model_attributes)\n",
    "model_afolu = ma.AFOLU(sa.model_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.model_attributes.get_baseline_scenario_id(sa.model_attributes.dim_strategy_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sa.model_attributes.dict_attributes[\"dim_time_series_id\"].table(\"dime\")\n",
    "sa.model_attributes.all_dims\n",
    "\n",
    "def get_baseline_scenario_id(dim: str):\n",
    "    \n",
    "    if dim not in sa.model_attributes.all_dims:\n",
    "        fpl = sf.format_print_list(sa.model_attributes.all_dims)\n",
    "        raise ValueError(f\"Invalid dimension '{dim}': valid dimensions are {fpl}.\")\n",
    "        \n",
    "    # get field to check\n",
    "    field_check = f\"baseline_{dim}\"\n",
    "    if field_check not in sa.model_attributes.dict_attributes[f\"dim_{dim}\"].table:\n",
    "        warnings.warn(f\"No baseline specified for dimension '{dim}'.\")\n",
    "        return None\n",
    "    else:\n",
    "        tab = sa.model_attributes.dict_attributes[f\"dim_{dim}\"].table\n",
    "        tab_red = list(tab[tab[field_check] == 1][dim])\n",
    "        \n",
    "        if len(tab_red) > 1:\n",
    "            raise ValueError(f\"Multiple baselines specified for dimension {dim}. Ensure that only baseline is set in the attribute table at '{tab.fp_table}'\")\n",
    "        \n",
    "        return tab_red[0]\n",
    "        \n",
    "    \n",
    "get_baseline_scenario_id(sa.model_attributes.dim_strategy_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsyme/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:462: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector', 'key_varreqs_all'\n",
      "  warnings.warn(f\"Invalid subsector attribute '{return_type}'. Valid return type values are:{valid_rts}\")\n"
     ]
    }
   ],
   "source": [
    "# get the\n",
    "tp = max(sa.model_attributes.get_time_periods()[0])\n",
    "f0, f1 = (f\"min_{tp}\", f\"max_{tp}\")\n",
    "df_template_afolu = pd.read_csv(os.path.join(sa.dir_ref, \"fake_data\", \"tmp_afolu_vars_with_mcmc_new.csv\"))\n",
    "df_template_afolu.drop([f0, f1], axis = 1, inplace = True)\n",
    "\n",
    "df_sampling_range_defaults = sa.model_attributes.build_default_sampling_range_df()\n",
    "\n",
    "df_template_afolu = pd.merge(df_template_afolu, df_sampling_range_defaults, how = \"left\", on = [\"variable\"])\n",
    "df_template_afolu = df_template_afolu.dropna(subset = [\"max_35\", \"min_35\"], how = \"any\")\n",
    "df_template_afolu.drop([\"src\"], axis = 1, inplace = True)\n",
    "\n",
    "df_template_afolu[\"time_series_id\"] = np.zeros(len(df_template_afolu)).astype(int)\n",
    "df_template_afolu[\"strategy_id\"] = np.zeros(len(df_template_afolu)).astype(int)\n",
    "df_template_afolu[\"uniform_scaling_q\"].iloc[~df_template_afolu[\"uniform_scaling_q\"].isna()] = df_template_afolu[\"uniform_scaling_q\"].iloc[2]\n",
    "\n",
    "# order\n",
    "tp_max = max(sa.model_attributes.get_time_periods()[0])\n",
    "fields_minmax = [f\"min_{tp_max}\", f\"max_{tp_max}\"]\n",
    "fields_prep = [x for x in df_template_afolu.columns if not x.isnumeric() and (x not in fields_minmax)] + fields_minmax\n",
    "fields_times = [x for x in df_template_afolu.columns if (x not in fields_prep)]\n",
    "\n",
    "df_template_afolu = df_template_afolu[fields_prep + fields_times]\n",
    "subsectors = [sa.model_attributes.dict_model_variable_to_subsector[sa.model_attributes.dict_variables_to_model_variables[x]] for x in df_template_afolu[\"variable\"]]\n",
    "df_template_afolu[\"subsector\"] = subsectors\n",
    "\n",
    "for sec in [\"AFOLU\", \"Socioeconomic\"]:\n",
    "\n",
    "    vars_keep = sa.model_attributes.get_variables_by_sector(sec, \"input\")\n",
    "    df_exp = df_template_afolu[df_template_afolu[\"variable\"].isin(vars_keep)].copy().reset_index(drop = True).drop_duplicates().sort_values(by = [\"subsector\", \"variable\"])\n",
    "    \n",
    "    dict_out = {\n",
    "        \"strategy_id-0\": df_exp, \n",
    "        \"strategy_id-1\": df_exp.iloc[0:0]\n",
    "    }\n",
    "    sf.dict_to_excel(sa.excel_template_path(sec, \"arg\", \"demo\", True), dict_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:910: UserWarning: clean_partial_category_dictionary: Invalid categories values 'domestic_rural_ww', 'domestic_urban_ww' dropped when cleaning the dictionary. Category values not found.\n",
      "  warnings.warn(f\"clean_partial_category_dictionary: Invalid categories values {missing_vals} dropped when cleaning the dictionary. Category values not found.\")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#    BUILD WASTE TEMPLATE    #\n",
    "##############################\n",
    "\n",
    "sec = \"Circular Economy\"\n",
    "l_vars = sa.model_attributes.get_variables_by_sector(sec, \"input\")\n",
    "l_vars.sort()\n",
    "\n",
    "df_base = df_exp.iloc[0:0].copy()\n",
    "df_base[\"variable\"] = l_vars\n",
    "# add subsectors\n",
    "subsectors = [sa.model_attributes.dict_model_variable_to_subsector[sa.model_attributes.dict_variables_to_model_variables[x]] for x in df_base[\"variable\"]]\n",
    "df_base[\"subsector\"] = subsectors\n",
    "df_base.drop([\"min_35\", \"max_35\"], axis = 1, inplace = True)\n",
    "df_base = pd.merge(df_base, df_sampling_range_defaults)\n",
    "\n",
    "df_safe = pd.read_excel(sa.excel_template_path(sec, \"arg\", \"demo\", True).replace(\"_demo.xlsx\", \"_demo_safe.xlsx\"))\n",
    "dft_out = pd.merge(df_base[[x for x in df_base.columns if (x != \"0\")]], df_safe[[\"variable\", \"0\"]].dropna(subset = [\"0\"]).reset_index(drop = True), how = \"left\")\n",
    "dft_out = dft_out[df_template_afolu.columns]\n",
    "\n",
    "if True:\n",
    "    \n",
    "\n",
    "\n",
    "    dict_out = {\n",
    "        \"strategy_id-0\": dft_out, \n",
    "        \"strategy_id-1\": dft_out.iloc[0:0]\n",
    "    }\n",
    "    sf.dict_to_excel(sa.excel_template_path(sec, \"arg\", \"demo\", True), dict_out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand later to integrate strategy dimension\n",
    "def build_modvar_input_db_from_templates(\n",
    "    model_attributes: ds.ModelAttributes,\n",
    "    sectors: list, \n",
    "    region: str, \n",
    "    template_type: str,\n",
    "    repl_missing_with_base: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    ##  run some checksee\n",
    "    \n",
    "    # check region\n",
    "    region = region.lower()\n",
    "    if region not in model_attributes.dict_attributes[\"region\"].key_values:\n",
    "        valid_regions = sf.format_print_list(model_attributes.dict_attributes[\"region\"].key_values)\n",
    "        raise ValueError(f\"Invalid region '{region}' specified. Valid regions are {valid_regions}\")\n",
    "        \n",
    "    # check sectors\n",
    "    sectors_drop = [x for x in sectors if (x not in model_attributes.all_sectors)]\n",
    "    if len(sectors_drop) > 0:\n",
    "        secs_drop = sf.format_print_list(sectors_drop)\n",
    "        raise ValueError(f\"Invalid sectors {secs_drop} found. Valid sectors are {model_attributes.all_sectors}.\")\n",
    "    \n",
    "    \n",
    "    ##\n",
    "    ##  TEMP: 0 only\n",
    "    ##\n",
    "    strat_base = 0\n",
    "    sheet_base = f\"{model_attributes.dim_strategy_id}-{strat_base}\"\n",
    "    \n",
    "    strats_eval = [strat_base]\n",
    "    \n",
    "    df_out = []\n",
    "    \n",
    "        \n",
    "    for sec in sectors:\n",
    "        fp_templ = sa.excel_template_path(sec, region, template_type, True)\n",
    "        if not os.path.exists(fp_templ):\n",
    "            raise ValueError(f\"Error: path '{fp_templ}' to template not found.\")\n",
    "        # check available sheets and ensure baseline is available\n",
    "        sheets_avail = pd.ExcelFile(fp_templ).sheet_names\n",
    "        if sheet_base not in sheets_avail:\n",
    "             raise ValueError(f\"Baseline strategy sheet {sheet_base} not found in '{fp_templ}'. The template must have a sheet for the baseline strategy.\")\n",
    "        \n",
    "        \n",
    "        for strat in strats_eval:\n",
    "            sheet = f\"{model_attributes.dim_strategy_id}-{strat}\"\n",
    "            if not sheet in sheets_avail:\n",
    "                msg = f\"Sheet {sheet} not found in '{fp_templ}'. Check the template.\"\n",
    "                if repl_missing_with_base:\n",
    "                    warnings.warn(f\"{msg}. The baseline strategy will be used.\")\n",
    "                    sheet = sheet_base\n",
    "                else:\n",
    "                    raise ValueError(msg)\n",
    "            \n",
    "            # \n",
    "            df_tmp = pd.read_excel(fp_templ, sheet_name = sheet)\n",
    "            df_tmp[model_attributes.dim_strategy_id] = strat\n",
    "            \n",
    "            #\n",
    "            #   ADD CHECKS FOR TIME PERIODS\n",
    "            #\n",
    "            \n",
    "            \n",
    "            #\n",
    "            #   ADD DIFFERENT STEPS FOR NON-BASELINE STRATEGY\n",
    "            #\n",
    "            \n",
    "            if len(df_out) == 0:\n",
    "                df_out.append(df_tmp)\n",
    "            else:\n",
    "                df_out.append(df_tmp[df_out[0].columns])\n",
    "                \n",
    "    df_out = pd.concat(df_out, axis = 0).sort_values(by = [\"subsector\", \"variable\"]).reset_index(drop = True)\n",
    "        \n",
    "    return df_out\n",
    "        \n",
    "# function to convert a model variable input database into a simple projection input dataframe\n",
    "def build_basic_df_from_modvar_inputs(\n",
    "    model_attributes: ds.ModelAttributes, \n",
    "    df_mv: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        build_basic_df_from_modvar_inputs will take a model input database and transform it (under baseline assumptions) into a projection dataframe.\n",
    "\n",
    "        - model_attributes: a ModelAttributes class\n",
    "\n",
    "        - df_mv: a dataframe of model variables\n",
    "    \"\"\"\n",
    "\n",
    "    df_mv_out = df_mv[[str(x) for x in model_attributes.get_time_periods()[0]]].transpose().reset_index(drop = True)\n",
    "    var_fields = list(df_mv[\"variable\"])\n",
    "    df_mv_out.rename(columns = dict(zip([(x) for x in range(len(df_mv_out.columns))], var_fields)), inplace = True)\n",
    "\n",
    "    df_mv_out[model_attributes.dim_time_period] = list(range(len(df_mv_out)))\n",
    "    var_fields.sort()\n",
    "\n",
    "    return df_mv_out[[model_attributes.dim_time_period] + var_fields]\n",
    "\n",
    "        \n",
    "df_mv_tmp = build_modvar_input_db_from_templates(sa.model_attributes, [\"Socioeconomic\", \"Circular Economy\", \"AFOLU\"], \"arg\", \"demo\")       \n",
    "df_mv_out = build_basic_df_from_modvar_inputs(sa.model_attributes, df_mv_tmp)\n",
    "df_mv_out.to_csv(os.path.join(sa.dir_ref, \"fake_data\", \"fake_data_complete.csv\"), index = None, encoding = \"UTF-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ef_trww_treated_aerobic_g_n2o_per_g_n',\n",
       " 'ef_trww_treated_anaerobic_g_n2o_per_g_n',\n",
       " 'ef_trww_treated_latrine_improved_g_n2o_per_g_n',\n",
       " 'ef_trww_treated_latrine_unimproved_g_n2o_per_g_n',\n",
       " 'ef_trww_treated_septic_g_n2o_per_g_n',\n",
       " 'ef_trww_untreated_no_sewerage_g_n2o_per_g_n',\n",
       " 'ef_trww_untreated_with_sewerage_g_n2o_per_g_n',\n",
       " 'frac_trww_tow_removed_treated_aerobic',\n",
       " 'frac_trww_tow_removed_treated_anaerobic',\n",
       " 'frac_trww_tow_removed_treated_latrine_improved',\n",
       " 'frac_trww_tow_removed_treated_latrine_unimproved',\n",
       " 'frac_trww_tow_removed_treated_septic',\n",
       " 'frac_trww_tow_removed_untreated_no_sewerage',\n",
       " 'frac_trww_tow_removed_untreated_with_sewerage',\n",
       " 'mcf_trww_treated_aerobic',\n",
       " 'mcf_trww_treated_anaerobic',\n",
       " 'mcf_trww_treated_latrine_improved',\n",
       " 'mcf_trww_treated_latrine_unimproved',\n",
       " 'mcf_trww_treated_septic',\n",
       " 'mcf_trww_untreated_no_sewerage',\n",
       " 'mcf_trww_untreated_with_sewerage']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in df_mv_out.columns if \"trww\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_safe[\"variable\"]) - set(dft_out[\"variable\"])\n",
    "l = list(set(dft_out[\"variable\"]) - set(df_safe[\"variable\"]))\n",
    "for k in l:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_safe = pd.read_excel(sa.excel_template_path(sec, \"arg\", \"demo\", True).replace(\"_demo.xlsx\", \"_demo_safe.xlsx\"))\n",
    "#dft = dict_out[\"strategy_id-0\"]\n",
    "#dft_out = pd.merge(dft[[x for x in dft.columns if (x != \"0\")]], df_safe[[\"variable\", \"0\"]].dropna(subset = [\"0\"]).reset_index(drop = True), how = \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.613846153846154"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = sa.model_attributes.dict_attributes[\"region\"].table.copy()\n",
    "dft[\"region\"] = [x.upper() for x in dft[\"region\"]]\n",
    "\n",
    "df_wb_waste = pd.read_csv(\"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/waste_management_data/wb_whatawaste_country_level_data_0.csv\")\n",
    "df_wb_waste = pd.merge(df_wb_waste, dft[[\"region\"]], left_on = [\"iso3c\"], right_on = [\"region\"], how = \"inner\")\n",
    "[x for x in df_wb_waste.columns if \"recy\" in x]\n",
    "\n",
    "np.mean(df_wb_waste[\"waste_treatment_recycling_percent\"].dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(df_wb_waste[[\"total_msw_total_msw_generated_tons_year\", \"population_population_number_of_people\"]])#.columns\n",
    "np.dot(arr[:, 0], arr[:, 1])/(sum(arr[:, 1])*sum(arr[:, 0]))\n",
    "               \n",
    "sum((arr[:, 1]*(arr[:, 0]/arr[:, 1]))/sum(arr[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "ind_waste = np.array(df_wb_waste[[x for x in df_wb_waste.columns if \"special_waste_industrial_waste_tons_year\" in x]].fillna(0).sum(axis = 1))\n",
    "gdp = (np.array(df_wb_waste[\"gdp\"])/1000)\n",
    "np.mean(ind_waste/gdp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v1 = np.array([12, 4, 10, 3, 3])\n",
    "v2 = np.array([4, 10, 1, 11, 2])\n",
    "#v2 = np.array([15, 4, 9, 3, 3])\n",
    "np.arccos(np.dot(v1, v2)/((np.dot(v1, v1)*np.dot(v2, v2))**(0.5)))/np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importlib.reload(sf)\n",
    "importlib.reload(ds)\n",
    "importlib.reload(sa)\n",
    "sa.excel_template_path(\"AFOLU\", \"arg\", \"demo\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexcel_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Sheet1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstartrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstartcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmerge_cells\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minf_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfreeze_panes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Write object to an Excel sheet.\n",
       "\n",
       "To write a single object to an Excel .xlsx file it is only necessary to\n",
       "specify a target file name. To write to multiple sheets it is necessary to\n",
       "create an `ExcelWriter` object with a target file name, and specify a sheet\n",
       "in the file to write to.\n",
       "\n",
       "Multiple sheets may be written to by specifying unique `sheet_name`.\n",
       "With all data written to the file it is necessary to save the changes.\n",
       "Note that creating an `ExcelWriter` object with a file name that already\n",
       "exists will result in the contents of the existing file being erased.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "excel_writer : str or ExcelWriter object\n",
       "    File path or existing ExcelWriter.\n",
       "sheet_name : str, default 'Sheet1'\n",
       "    Name of sheet which will contain DataFrame.\n",
       "na_rep : str, default ''\n",
       "    Missing data representation.\n",
       "float_format : str, optional\n",
       "    Format string for floating point numbers. For example\n",
       "    ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n",
       "columns : sequence or list of str, optional\n",
       "    Columns to write.\n",
       "header : bool or list of str, default True\n",
       "    Write out the column names. If a list of string is given it is\n",
       "    assumed to be aliases for the column names.\n",
       "index : bool, default True\n",
       "    Write row names (index).\n",
       "index_label : str or sequence, optional\n",
       "    Column label for index column(s) if desired. If not specified, and\n",
       "    `header` and `index` are True, then the index names are used. A\n",
       "    sequence should be given if the DataFrame uses MultiIndex.\n",
       "startrow : int, default 0\n",
       "    Upper left cell row to dump data frame.\n",
       "startcol : int, default 0\n",
       "    Upper left cell column to dump data frame.\n",
       "engine : str, optional\n",
       "    Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n",
       "    via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n",
       "    ``io.excel.xlsm.writer``.\n",
       "merge_cells : bool, default True\n",
       "    Write MultiIndex and Hierarchical Rows as merged cells.\n",
       "encoding : str, optional\n",
       "    Encoding of the resulting excel file. Only necessary for xlwt,\n",
       "    other writers support unicode natively.\n",
       "inf_rep : str, default 'inf'\n",
       "    Representation for infinity (there is no native representation for\n",
       "    infinity in Excel).\n",
       "verbose : bool, default True\n",
       "    Display more information in the error logs.\n",
       "freeze_panes : tuple of int (length 2), optional\n",
       "    Specifies the one-based bottommost row and rightmost column that\n",
       "    is to be frozen.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "ExcelWriter : Class for writing DataFrame objects into excel sheets.\n",
       "read_excel : Read an Excel file into a pandas DataFrame.\n",
       "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "For compatibility with :meth:`~DataFrame.to_csv`,\n",
       "to_excel serializes lists and dicts to strings before writing.\n",
       "\n",
       "Once a workbook has been saved it is not possible write further data\n",
       "without rewriting the whole workbook.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       "Create, write to and save a workbook:\n",
       "\n",
       ">>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
       "...                    index=['row 1', 'row 2'],\n",
       "...                    columns=['col 1', 'col 2'])\n",
       ">>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n",
       "\n",
       "To specify the sheet name:\n",
       "\n",
       ">>> df1.to_excel(\"output.xlsx\",\n",
       "...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n",
       "\n",
       "If you wish to write to more than one sheet in the workbook, it is\n",
       "necessary to specify an ExcelWriter object:\n",
       "\n",
       ">>> df2 = df1.copy()\n",
       ">>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n",
       "...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n",
       "...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n",
       "\n",
       "ExcelWriter can also be used to append to an existing Excel file:\n",
       "\n",
       ">>> with pd.ExcelWriter('output.xlsx',\n",
       "...                     mode='a') as writer:  # doctest: +SKIP\n",
       "...     df.to_excel(writer, sheet_name='Sheet_name_3')\n",
       "\n",
       "To set the library that is used to write the Excel file,\n",
       "you can pass the `engine` keyword (the default engine is\n",
       "automatically chosen depending on the file extension):\n",
       "\n",
       ">>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "?pd.DataFrame.to_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'af'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.model_attributes.get_sector_attribute(\"AFOLU\", \"abbreviation_sector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ref',\n",
       " '.DS_Store',\n",
       " 'LICENSE',\n",
       " 'project_metadata.json',\n",
       " 'python',\n",
       " 'out',\n",
       " 'pyproject.toml',\n",
       " 'docs',\n",
       " 'model_overview.md',\n",
       " 'README.md',\n",
       " '.gitignore',\n",
       " 'julia',\n",
       " 'README.rst',\n",
       " '.git']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(sa.dir_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
